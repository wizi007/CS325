
# Description
This scripts uses stored prompts for a local LLM (phi3) from a text file and generates the responses in another text file
## Prerequisite
Download ollama using this link https://ollama.com/download

## Setup
To call the phi-3 API generated by ollama, run `ollama serve`

Create a new virtual environment and install the dependency packages by running `pip install --file requirements.yaml`

Begin the ollama phi3 model with `ollama run phi3`

## Running the script
Run the script in the environment with the command `python main.py`



